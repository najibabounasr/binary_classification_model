# binary_classification_model
A neural network powered model, used to help VC funds sort through funding applications.  

# Background

To predict whether Alphabet Soup funding applicants will be successful, we can create a binary classification model using a deep neural network.

The program uses a CSV file containing more than 34,000 organizations that have received funding from a VC firm over the years. The CSV file contains a variety of information about each business, including whether or not it ultimately became successful. With your knowledge of machine learning and neural networks, you decide to use the features in the provided dataset to create a binary classifier model that will predict whether an applicant will become a successful business.

## Features and Implementation - Vision and Results:

 In a bid to drive revenue, you’ll produce a Jupyter notebook that contains your data preparation, analysis, and visualizations for all the time series data that the company needs to understand. You’ll use text and comments to document your findings, and you’ll answer the question prompts in the instructions. Specifically, this file should contain the following:

Visual depictions of seasonality (as measured by Google Search traffic) that are of interest to the company.

An evaluation of how the company’s stock price correlates to its Google Search traffic.

A Prophet forecast model that can predict hourly user search traffic.

Answers to questions in the instructions that you write in your Jupyter Notebook.

(Optional) A plot of a forecast for the company’s future revenue.

Push your final notebook to your GitHub repository so that others can review your work.


## Instructions
The steps for this challenge are broken out into the following sections:

    - Prepare the data for use on a neural network model.

    - Compile and evaluate a binary classification model using a neural network.

    - Optimize the neural network model.


## Technologies

This Project utilizes Google Colab as the managed runtime hosted environment. All required libraries and dependencies can be installed 
on the Google Colab session -- meaning most installation occurs within
the notebook.

    - Google Colab - managed runtime hosted environment to write and execute code on the cloud -- suited for machine learning programs
    with dependencies and modules incompatibile with the local machine.

**Required Libraries:**

Key libraries used in the project :

    - [pandas](https://github.com/pandas-dev/pandas) 
     pandas is used to interact with data packages, plot data frames, create new dataframes, describe abailable data, and helps traders and fintech proffesionals organize financial data to perform advanced decisionmaking. 

    - [Tensorflow](https://github.com/tensorflow/tensorflow)
     TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.

    - [scikit-learn](https://pypi.org/project/scikit-learn/)
     scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.


## Usage

This application can be accessed by accessing google colab, by connecting the repository file on your local computer to your google Drive account, you can access the Google Colab notebook.
## Contributors

The sole contributor for this project is:

**NAJIB ABOU NASR**
 no instagram or linkedin yet!
---

## License

Using the 'MIT' license!
--- 

## History

### 
    Here, I documented my command line inputs, to show the changes I had made, and document the debugging and programing process:  
---

